import numpy as np
import matplotlib.pyplot as plt

def make_very_easy_means(K):
    """
    Very easy stochastic bandit:
    arm 0 has mean 0.9, all others mean 0.1.
    """
    means = np.full(K, 0.1, dtype=float)
    means[0] = 0.9
    return means

# ============================================================
#  Tsallis / DOPA probabilities 
# ============================================================


def tsallis_probs(u, eta, q=0.5, n_iter=60, eps=1e-12):
    '''
    Compute arm-sampling probabilities for a Fréchet ambiguity set
    generated by a Tsallis / shifted-Pareto generator F.

    For q = 0.5 this matches the DOPA / Tsallis-INF mapping with
    marginal generator F(s) = (2 - s)^(-2).

    Parameters
    ----------
    u : array_like, shape (K,)
        Cumulative (importance-weighted) reward estimates.
    eta : float
        Scale parameter of the Fréchet marginal.
        For BOBW Tsallis-INF / DOPA: eta_t = 2 * sqrt(t).
    q : float, optional
        Tsallis parameter (q in (0,1] in the theory; q=0.5 is the main case).
    n_iter : int, optional
        Number of bisection iterations.
    eps : float, optional
        Small constant for numerical stability.
    '''
    u = np.asarray(u, dtype=float)
    K = u.shape[0]

    # Marginal generator F; for q=0.5: F(s) = (2 - s)^(-2)
    def F(s):
        base = s * (q - 1.0) / q + 1.0 / q  # affine transform
        base = np.maximum(base, eps)
        return base ** (1.0 / (q - 1.0))

    # Fréchet marginal CDF F_k(s) = 1 - F(-s / eta), clipped to [0,1]
    def F_k(s):
        return np.clip(1.0 - F(-s / float(eta)), 0.0, 1.0)

    # Inverse generator F^{-1}
    def F_inv(t):
        t = np.maximum(t, eps)
        return (q * (t ** (q - 1.0)) - 1.0) / (q - 1.0)

    # F_k^{-1}(1 - 1/K) = -eta * F^{-1}(1/K)
    Fk_inv_1_minus_1_over_K = -eta * F_inv(1.0 / K)

    offset = Fk_inv_1_minus_1_over_K
    tau_u = np.max(-u - offset)
    tau_l = np.min(-u - offset)

    if tau_u < tau_l:
        tau_u, tau_l = tau_l, tau_u

    # Bisection to find tau such that sum_k (1 - F_k(-u_k - tau)) = 1
    tau = 0.5 * (tau_u + tau_l)
    for _ in range(n_iter):
        tau = 0.5 * (tau_u + tau_l)
        p_hat = 1.0 - F_k(-u - tau)
        if p_hat.sum() > 1.0:
            tau_u = tau
        else:
            tau_l = tau

    # Final probs
    F_vals = F_k(-u - tau)
    S = F_vals.sum()
    p = (1.0 + S) / K - F_vals

    # Numerical stability
    p = np.maximum(p, 0.0)
    s = p.sum()
    if s <= 0:
        p = np.ones_like(p) / K
    else:
        p /= s

    return p


# ============================================================
#  Exponential FTRL: softmax 
# ============================================================

def softmax_probs(u, eta):
    '''
    Softmax arm-sampling probabilities (Exp3-style).

    u : array_like, cumulative reward estimates
    eta : float, learning rate / temperature
    '''
    u = np.asarray(u, dtype=float)
    z = u / float(eta)
    z -= z.max()  # numerical stability
    exp_z = np.exp(z)
    return exp_z / exp_z.sum()


# ============================================================
#  Environments
# ============================================================

def make_stochastic_means(K, gap=0.1):
    '''
    Simple stochastic bandit: means = 0.9, 0.9-gap, 0.9-2*gap, ...
    clipped to [0.05, 0.95].
    '''
    base = 0.9
    means = base - gap * np.arange(K)
    means = np.clip(means, 0.05, 0.95)
    return means


def make_adversarial_rewards(T, K):
    '''
    Simple adversarial (non-stationary) bandit:
    - For t even (0-based): arm 0 has reward 1, others 0
    - For t odd:            arm 1 has reward 1, others 0
    Remaining arms (2,...,K-1) always 0.

    Best fixed arm gets about T/2 reward.
    '''
    R = np.zeros((T, K), dtype=float)
    for t in range(T):
        if t % 2 == 0:
            R[t, 0] = 1.0
        else:
            R[t, 1] = 1.0
    return R


# ============================================================
#  Simulation: stochastic environment
# ============================================================

def run_stochastic(
    T=5000,
    K=5,
    n_runs=20,
    gap=0.1,
    eta_exp=1.0,
    eta_uni=1.0,
    seed=0,
):
    '''
    Run stochastic bandit experiment.

    - Tsallis / DOPA policy: q = 0.5, eta_t = 2 * sqrt(t)  (BOBW schedule)
    - Exp3-style baseline (softmax): constant eta_exp
    - Tsallis q = 2 baseline: constant eta_uni
    '''
    means = make_very_easy_means(K)
    mu_star = means.max()

    regrets_tsallis_runs = np.zeros((n_runs, T))
    regrets_exp_runs = np.zeros((n_runs, T))
    regrets_uni_runs = np.zeros((n_runs, T))

    for run in range(n_runs):
        rng = np.random.default_rng(seed + run)
        u_tsallis = np.zeros(K)
        u_exp = np.zeros(K)
        u_uni = np.zeros(K)

        cum_reg_tsallis = 0.0
        cum_reg_exp = 0.0
        cum_reg_uni = 0.0

        for t in range(1, T + 1):
            # --- Tsallis / DOPA (q = 0.5, eta_t = 2 sqrt(t)) ---
            # eta_t_tsallis = 2.0 * np.sqrt(T)
            alpha = 0.5
            eta_t_tsallis = np.sqrt(T * (1 - alpha) / (2 * alpha)) * (K ** (alpha - 0.5))

            p_t = tsallis_probs(u_tsallis, eta=eta_t_tsallis, q=0.5)
            a_t = rng.choice(K, p=p_t)
            r_t = 1.0 if rng.random() < means[a_t] else 0.0

            # shift to [-1,0] for the update (we are shifting regret to match our theory)
            r_t_bandit = r_t - 1.0
            u_tsallis[a_t] += r_t_bandit / p_t[a_t]

            cum_reg_tsallis += mu_star - means[a_t]
            regrets_tsallis_runs[run, t - 1] = cum_reg_tsallis

            # --- Exponential ---
            p_e = softmax_probs(u_exp, eta=eta_exp)
            a_e = rng.choice(K, p=p_e)
            r_e = 1.0 if rng.random() < means[a_e] else 0.0
            r_e_bandit = r_e - 1.0
            u_exp[a_e] += r_e_bandit / p_e[a_e]

            cum_reg_exp += mu_star - means[a_e]
            regrets_exp_runs[run, t - 1] = cum_reg_exp

            # --- Tsallis with q = 2 (Uniform) ---
            p_u = tsallis_probs(u_uni, eta=eta_uni, q=2.0)
            a_u = rng.choice(K, p=p_u)
            r_u = 1.0 if rng.random() < means[a_u] else 0.0
            r_u_bandit = r_u - 1.0
            u_uni[a_u] += r_u_bandit / p_u[a_u]

            cum_reg_uni += mu_star - means[a_u]
            regrets_uni_runs[run, t - 1] = cum_reg_uni

    sto_tsallis_mean = regrets_tsallis_runs.mean(axis=0)
    sto_tsallis_std = regrets_tsallis_runs.std(axis=0)
    sto_exp_mean = regrets_exp_runs.mean(axis=0)
    sto_exp_std = regrets_exp_runs.std(axis=0)
    sto_uni_mean = regrets_uni_runs.mean(axis=0)
    sto_uni_std = regrets_uni_runs.std(axis=0)

    return (
        sto_tsallis_mean,
        sto_tsallis_std,
        sto_exp_mean,
        sto_exp_std,
        sto_uni_mean,
        sto_uni_std,
    )


# ============================================================
#  Simulation: adversarial environment
# ============================================================

def run_adversarial(
    T=5000,
    K=5,
    n_runs=20,
    eta_exp=1.0,
    eta_uni=1.0,
    seed=0,
):
    '''
    Run adversarial bandit experiment.

    - Tsallis / DOPA policy: q = 0.5, eta_t = 2 * sqrt(t)
    - Exp3 baseline: constant eta_exp
    - Tsallis q = 2 baseline: constant eta_uni
    '''
    R = make_adversarial_rewards(T, K)
    cum_per_arm = R.sum(axis=0)
    k_star = np.argmax(cum_per_arm)
    best_cum = np.cumsum(R[:, k_star])

    regrets_tsallis_runs = np.zeros((n_runs, T))
    regrets_exp_runs = np.zeros((n_runs, T))
    regrets_uni_runs = np.zeros((n_runs, T))

    for run in range(n_runs):
        rng = np.random.default_rng(seed + 1000 + run)

        u_tsallis = np.zeros(K)
        u_exp = np.zeros(K)
        u_uni = np.zeros(K)

        cum_alg_tsallis = 0.0
        cum_alg_exp = 0.0
        cum_alg_uni = 0.0

        for t in range(1, T + 1):
            # --- Exponential---
            p_e = softmax_probs(u_exp, eta=eta_exp)
            a_e = rng.choice(K, p=p_e)
            r_e = R[t - 1, a_e]
            r_e_bandit = r_e - 1.0
            u_exp[a_e] += r_e_bandit / p_e[a_e]

            cum_alg_exp += r_e
            regrets_exp_runs[run, t - 1] = best_cum[t - 1] - cum_alg_exp

            # --- Tsallis / DOPA (q = 0.5, eta_t = 2 sqrt(t)) ---
            # eta_t_tsallis = 2.0 * np.sqrt(T)
            alpha = 0.5
            eta_t_tsallis = np.sqrt(T * (1 - alpha) / (2 * alpha)) * (K ** (alpha - 0.5))

            p_t = tsallis_probs(u_tsallis, eta=eta_t_tsallis, q=0.5)
            a_t = rng.choice(K, p=p_t)
            r_t = R[t - 1, a_t]
            r_t_bandit = r_t - 1.0
            u_tsallis[a_t] += r_t_bandit / p_t[a_t]

            cum_alg_tsallis += r_t
            regrets_tsallis_runs[run, t - 1] = best_cum[t - 1] - cum_alg_tsallis

            # --- Tsallis q = 2 baseline ---
            p_u = tsallis_probs(u_uni, eta=eta_uni, q=2.0)
            a_u = rng.choice(K, p=p_u)
            r_u = R[t - 1, a_u]
            r_u_bandit = r_u - 1.0
            u_uni[a_u] += r_u_bandit / p_u[a_u]

            cum_alg_uni += r_u
            regrets_uni_runs[run, t - 1] = best_cum[t - 1] - cum_alg_uni

    adv_tsallis_mean = regrets_tsallis_runs.mean(axis=0)
    adv_tsallis_std = regrets_tsallis_runs.std(axis=0)
    adv_exp_mean = regrets_exp_runs.mean(axis=0)
    adv_exp_std = regrets_exp_runs.std(axis=0)
    adv_uni_mean = regrets_uni_runs.mean(axis=0)
    adv_uni_std = regrets_uni_runs.std(axis=0)

    return (
        adv_tsallis_mean,
        adv_tsallis_std,
        adv_exp_mean,
        adv_exp_std,
        adv_uni_mean,
        adv_uni_std,
    )


# ============================================================
#  Main script
# ============================================================

if __name__ == "__main__":
    T = 10_000
    K = 5
    n_runs = 10

    # These are only used for the baselines; Tsallis / DOPA uses eta_t = 2 sqrt(t)
    eta_exp = np.sqrt(T)
    eta_uni = np.sqrt(T)

    seed = 1234
    gap_sto = 0.1

    # --- Stochastic experiment ---
    (
        sto_tsallis_mean,
        sto_tsallis_std,
        sto_exp_mean,
        sto_exp_std,
        sto_uni_mean,
        sto_uni_std,
    ) = run_stochastic(
        T=T,
        K=K,
        n_runs=n_runs,
        gap=gap_sto,
        eta_exp=eta_exp,
        eta_uni=eta_uni,
        seed=seed,
    )

    # --- Adversarial experiment ---
    (
        adv_tsallis_mean,
        adv_tsallis_std,
        adv_exp_mean,
        adv_exp_std,
        adv_uni_mean,
        adv_uni_std,
    ) = run_adversarial(
        T=T,
        K=K,
        n_runs=n_runs,
        eta_exp=eta_exp,
        eta_uni=eta_uni,
        seed=seed,
    )

    t = np.arange(1, T + 1)
    z = 1.96  # std is chosen like that for 95% confidence interval

    plt.figure(figsize=(10, 10))

    # =======================
    # Stochastic
    # =======================
    plt.subplot(2, 1, 1)

    se_uni = sto_uni_std / np.sqrt(n_runs)
    plt.plot(t, sto_uni_mean, label="Tsallis (q=2)", color="gray")
    plt.fill_between(
        t,
        sto_uni_mean - z * se_uni,
        sto_uni_mean + z * se_uni,
        alpha=0.2,
        color="gray",
    )

    se_exp = sto_exp_std / np.sqrt(n_runs)
    plt.plot(t, sto_exp_mean, label="Exponential",
             linestyle="--", color="green")
    plt.fill_between(
        t,
        sto_exp_mean - z * se_exp,
        sto_exp_mean + z * se_exp,
        alpha=0.2,
        color="green",
    )

    se_ts = sto_tsallis_std / np.sqrt(n_runs)
    plt.plot(t, sto_tsallis_mean, label="Tsallis (q=1/2)",
             linestyle="-.", color="purple")
    plt.fill_between(
        t,
        sto_tsallis_mean - z * se_ts,
        sto_tsallis_mean + z * se_ts,
        alpha=0.2,
        color="purple",
    )

    plt.title("Stochastic environment")
    plt.xlabel("Round t")
    plt.ylabel("Cumulative regret")
    plt.legend()
    plt.grid(True, alpha=0.3)

    # =======================
    # Adversarial
    # =======================
    plt.subplot(2, 1, 2)

    se_uni = adv_uni_std / np.sqrt(n_runs)
    plt.plot(t, adv_uni_mean, label="DOPA Tsallis (q=2)", color="gray")
    plt.fill_between(
        t,
        adv_uni_mean - z * se_uni,
        adv_uni_mean + z * se_uni,
        alpha=0.2,
        color="gray",
    )

    se_exp = adv_exp_std / np.sqrt(n_runs)
    plt.plot(t, adv_exp_mean, label="DOPA Exponential",
             linestyle="--", color="green")
    plt.fill_between(
        t,
        adv_exp_mean - z * se_exp,
        adv_exp_mean + z * se_exp,
        alpha=0.2,
        color="green",
    )

    se_ts = adv_tsallis_std / np.sqrt(n_runs)
    plt.plot(t, adv_tsallis_mean, label="DOPA Tsallis (q=1/2)",
             linestyle="-.", color="purple")
    plt.fill_between(
        t,
        adv_tsallis_mean - z * se_ts,
        adv_tsallis_mean + z * se_ts,
        alpha=0.2,
        color="purple",
    )

    plt.title("Adversarial environment")
    plt.xlabel("Round t")
    plt.ylabel("Cumulative regret")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("regret_plot.pdf", bbox_inches="tight")
    plt.show()
